{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2680055,"sourceType":"datasetVersion","datasetId":2946}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\nimport cv2\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm  # For progress bars\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Load Dataset ---\n","metadata":{}},{"cell_type":"code","source":"# Define your dataset paths (adjust the paths as needed)\ndataset_dirs = [\n    '/kaggle/input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_1/youtube_faces_with_keypoints_full_1',\n    '/kaggle/input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_2/youtube_faces_with_keypoints_full_2',\n    '/kaggle/input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_3/youtube_faces_with_keypoints_full_3',\n    '/kaggle/input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_4/youtube_faces_with_keypoints_full_4'\n]\n\n# Collecting all files from the specified directories\nfiles = []\nfor dataset_dir in dataset_dirs:\n    files += [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith('.npz')]\n\n# Debugging step: print the number of files found\nprint(f\"Number of files found: {len(files)}\")\nif len(files) == 0:\n    print(\"No files found! Please check the directory paths and file structure.\")\nelse:\n    # Perform train-test split if files are found\n    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n\n    print(f\"Training set size: {len(train_files)} files\")\n    print(f\"Validation set size: {len(val_files)} files\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Data Generator Class ---\n","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, files, batch_size=32, sample_ratio=0.1, img_size=(256, 256), shuffle=True, return_names=False):\n        self.files = files\n        self.batch_size = batch_size\n        self.sample_ratio = sample_ratio\n        self.img_size = img_size\n        self.shuffle = shuffle\n        self.return_names = return_names\n        \n        if self.shuffle:\n            random.shuffle(self.files)\n\n    def __len__(self):\n        return int(np.floor(len(self.files) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_files = self.files[index * self.batch_size:(index + 1) * self.batch_size]\n\n        images, bboxes, landmarks_2d, landmarks_3d, image_names = [], [], [], [], []\n        for npz_file in batch_files:\n            data = np.load(npz_file)\n            color_images = data['colorImages']\n            bboxes_data = data['boundingBox']\n            landmarks2D_data = data['landmarks2D']\n            landmarks3D_data = data['landmarks3D']\n\n            num_frames = color_images.shape[-1]\n            sampled_indices = random.sample(range(num_frames), int(self.sample_ratio * num_frames))\n\n            filename = os.path.basename(npz_file).split('.')[0]\n\n            for idx in sampled_indices:\n                img = color_images[..., idx]\n                img = cv2.resize(img, self.img_size)\n                img = img / 255.0  # Normalize\n\n                images.append(img)  # Use original images\n\n                bboxes.append(bboxes_data[..., idx])\n                landmarks_2d.append(landmarks2D_data[..., idx])\n                landmarks_3d.append(landmarks3D_data[..., idx])\n                image_names.append(filename)\n\n        images = np.array(images)\n\n        if self.return_names:\n            return images, images, image_names  # Return image names for visualization\n        else:\n            return images, images  # Don't return names during training\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            random.shuffle(self.files)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Data Generators ---\n","metadata":{}},{"cell_type":"code","source":"train_generator = DataGenerator(files=train_files, batch_size=8, sample_ratio=0.05, img_size=(256, 256), shuffle=True)\nval_generator = DataGenerator(files=val_files, batch_size=8, sample_ratio=0.05, img_size=(256, 256), shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- SR-GAN Model Components ---\n","metadata":{}},{"cell_type":"code","source":"def build_generator():\n    input_img = layers.Input(shape=(256, 256, 3))\n\n    # Initial Convolutional Layer\n    x = layers.Conv2D(64, (9, 9), padding='same')(input_img)\n    x = layers.Activation('relu')(x)\n\n    # Residual Blocks\n    for _ in range(16):\n        res = layers.Conv2D(64, (3, 3), padding='same')(x)\n        res = layers.BatchNormalization()(res)\n        res = layers.Activation('relu')(res)\n        res = layers.Conv2D(64, (3, 3), padding='same')(res)\n        res = layers.BatchNormalization()(res)\n        x = layers.add([x, res])\n\n    # Upsampling Layers\n    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n    x = layers.UpSampling2D(size=(2, 2))(x)\n    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n    x = layers.UpSampling2D(size=(2, 2))(x)\n\n    # Final Convolutional Layer\n    x = layers.Conv2D(3, (9, 9), padding='same')(x)\n    output_img = layers.Activation('tanh')(x)\n\n    generator = keras.Model(inputs=input_img, outputs=output_img, name=\"Generator\")\n    return generator\n\ndef build_discriminator():\n    input_img = layers.Input(shape=(1024, 1024, 3))\n\n    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_img)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n\n    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n\n    x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n\n    x = layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n\n    x = layers.Flatten()(x)\n    x = layers.Dense(1024)(x)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n    validity = layers.Dense(1, activation='sigmoid')(x)\n\n    discriminator = keras.Model(inputs=input_img, outputs=validity, name=\"Discriminator\")\n    return discriminator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Build SR-GAN ---\n","metadata":{}},{"cell_type":"code","source":"generator = build_generator()\ndiscriminator = build_discriminator()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile Models\ndiscriminator.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\ndiscriminator.trainable = False  # Freeze the discriminator during generator training","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combined Model\ninput_img = layers.Input(shape=(256, 256, 3))\ngenerated_img = generator(input_img)\nvalidity = discriminator(generated_img)\n\ncombined_model = keras.Model(inputs=input_img, outputs=[generated_img, validity])\ncombined_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5), loss=['mean_squared_error', 'binary_crossentropy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Training Loop with Callbacks ---\n","metadata":{}},{"cell_type":"code","source":"# Callbacks\ncallbacks = [\n    ModelCheckpoint('/kaggle/working/best_srgan_model.keras', save_best_only=True, monitor='val_loss', mode='min'),\n    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n]\n\n# Training Loop\nepochs = 50\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n\n    for batch in tqdm(train_generator):\n        # Generate high-resolution images from low-resolution images\n        low_res_images, _ = batch\n        high_res_images = low_res_images  # Use the same images for training the generator\n\n        # Train the Discriminator\n        valid = np.ones((len(low_res_images), 1))  # Real labels\n        fake = np.zeros((len(low_res_images), 1))  # Fake labels\n\n        d_loss_real = discriminator.train_on_batch(high_res_images, valid)\n        d_loss_fake = discriminator.train_on_batch(generator.predict(low_res_images), fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the Generator\n        g_loss = combined_model.train_on_batch(low_res_images, [high_res_images, valid])\n\n    print(f\"[Epoch {epoch + 1}/{epochs}] [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss[0]} | G validity loss: {g_loss[1]}]\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# --- Visualize Results ---\n","metadata":{}},{"cell_type":"code","source":"def visualize_results(generator, low_res_images, image_names, n=5):\n    plt.figure(figsize=(20, 10))\n    for i in range(n):\n        plt.subplot(2, n, i + 1)\n        plt.imshow(low_res_images[i])\n        plt.title(f\"Low-Resolution Image\\n{image_names[i]}\")\n        plt.axis(\"off\")\n\n        high_res_image = generator.predict(low_res_images[i:i + 1])\n        plt.subplot(2, n, i + 1 + n)\n        plt.imshow(high_res_image[0])\n        plt.title(f\"Generated High-Resolution Image\\n{image_names[i]}\")\n        plt.axis(\"off\")\n    plt.show()\n\n# Sample results from the training\nsample_batch = train_generator[0]  # Get the first batch\nsample_low_res_images, sample_image_names = sample_batch  # Get both images and names\nvisualize_results(generator, sample_low_res_images, sample_image_names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the generator model\ngenerator.save('/kaggle/working/srgan_generator_model.keras')","metadata":{},"execution_count":null,"outputs":[]}]}