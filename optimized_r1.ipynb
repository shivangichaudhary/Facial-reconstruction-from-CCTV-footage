{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Reconstruction from Low-Quality CCTV Footage\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to develop a machine learning solution for reconstructing human faces from low-quality CCTV footage. The solution will address issues like motion blur, poor lighting, and low resolution to help investigators identify suspects more accurately.\n",
    "\n",
    "## Objectives for Round 1\n",
    "1. **Basic Facial Reconstruction Model:** Develop a model to reconstruct or enhance facial images from low-quality CCTV footage.\n",
    "2. **Image Enhancement Techniques:** Apply methods like super-resolution, noise reduction, or deblurring.\n",
    "3. **Comparison of Results:** Provide side-by-side comparisons of original and enhanced images.\n",
    "\n",
    "## Dataset Overview\n",
    "The dataset consists of .npz files containing video frames, bounding boxes, and landmarks.\n",
    "\n",
    "### Example Files:\n",
    "- **Aaron_Eckhart_0.npz**\n",
    "  - `colorImages.npy (231,237,3,79)`\n",
    "  - `boundingBox.npy (4,2,79)`\n",
    "  - `landmarks2D.npy (68,2,79)`\n",
    "  - `landmarks3D.npy (68,3,79)`\n",
    "\n",
    "## Step 1: Data Loading and Preprocessing\n",
    "First, we will load the data and preprocess it for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load all files in the dataset directory\n",
    "dataset_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the target shape for images\n",
    "TARGET_SHAPE = (64, 64, 3)\n",
    "\n",
    "def resize_image(image):\n",
    "    return resize(image, TARGET_SHAPE, preserve_range=True, anti_aliasing=True)\n",
    "\n",
    "def load_npz_file(file_path):\n",
    "    data = np.load(file_path, mmap_mode='r')\n",
    "    color_images = data['colorImages'] / 255.0  # Normalize images\n",
    "    # Resize images one at a time to minimize memory usage\n",
    "    color_images_resized = np.array([resize_image(img) for img in color_images.transpose(3, 0, 1, 2)])\n",
    "    \n",
    "    bounding_boxes = data['boundingBox']\n",
    "    landmarks_2d = data['landmarks2D']\n",
    "    landmarks_3d = data['landmarks3D']\n",
    "    \n",
    "    return color_images_resized, bounding_boxes, landmarks_2d, landmarks_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith('.npz')]\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def data_generator(file_list, batch_size=4):  # Reduced batch size for low-end systems\n",
    "    while True:\n",
    "        for i in range(0, len(file_list), batch_size):\n",
    "            batch_files = file_list[i:i + batch_size]\n",
    "            images = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced threads for system stability\n",
    "                results = list(tqdm(executor.map(load_npz_file, batch_files), total=len(batch_files), desc=\"Loading batch\"))\n",
    "            \n",
    "            for result in results:\n",
    "                color_images, _, _, _ = result\n",
    "                images.append(color_images)\n",
    "            \n",
    "            # Free memory by discarding the original arrays after concatenation\n",
    "            images = np.concatenate(images, axis=0)\n",
    "            augmented_images = np.array([datagen.random_transform(img) for img in images])\n",
    "            \n",
    "            yield augmented_images, augmented_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split file list into training and validation sets\n",
    "train_files, val_files = train_test_split(file_list, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Model Development\n",
    "We will develop a Convolutional Neural Network (CNN) for facial reconstruction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, MaxPooling2D,Dropout, concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)  # reduced filters from 64 to 32\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)  # reduced filters from 128 to 64\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)  # reduced filters from 256 to 128\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    \n",
    "    # Decoder\n",
    "    up4 = UpSampling2D((2, 2))(conv3)\n",
    "    concat4 = concatenate([up4, conv2])\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(concat4)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    up5 = UpSampling2D((2, 2))(conv4)\n",
    "    concat5 = concatenate([up5, conv1])\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(concat5)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(conv5)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = TARGET_SHAPE\n",
    "model = build_unet_model(input_shape)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3: Training the Model\n",
    "We will train the model using the training data and validate it using the validation data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint('models/unet_model.keras', save_best_only=True, save_freq='epoch')\n",
    "]\n",
    "\n",
    "# Train model\n",
    "train_gen = data_generator(train_files, batch_size=8)\n",
    "val_gen = data_generator(val_files, batch_size=8)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=len(train_files) // 8,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=len(val_files) // 8,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation and Comparison\n",
    "We will evaluate the model and compare the original and enhanced images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_weights('models/unet_model.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "val_gen = data_generator(val_files, batch_size=32)\n",
    "val_loss, val_accuracy = model.evaluate(val_gen, steps=len(val_files) // 32)\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(original, enhanced, n=5):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(n):\n",
    "        # Original images\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i])\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Enhanced images\n",
    "        plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(enhanced[i])\n",
    "        plt.title('Enhanced')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Predict enhanced images\n",
    "val_gen = data_generator(val_files, batch_size=32)\n",
    "original_images, _ = next(val_gen)\n",
    "enhanced_images = model.predict(original_images)\n",
    "\n",
    "# Display images\n",
    "display_images(original_images, enhanced_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*`This notebook provides a comprehensive approach to developing a facial reconstruction model from scratch, including data loading, preprocessing, model development, training, and evaluation. It also includes optimizations and debugging techniques to handle the large dataset efficiently.`*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
